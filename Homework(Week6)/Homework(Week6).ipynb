{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6786a97e",
   "metadata": {},
   "source": [
    "### Homework\n",
    "### 李嘉琛 2301212345"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97601c50",
   "metadata": {},
   "source": [
    "#### Extract the first page of all pdfs in the \"mypdfs \" folder and merge them into one pdf file named \"new.pdf\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ed86db6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WindowsPath('mypdfs/1905.05406.pdf'), WindowsPath('mypdfs/1908.02426.pdf'), WindowsPath('mypdfs/2103.07579.pdf'), WindowsPath('mypdfs/2106.13112.pdf'), WindowsPath('mypdfs/2109.05673.pdf')]\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "path = Path(r\"mypdfs\")\n",
    "pdf_files = [p for p in path.glob(\"*.pdf\")]\n",
    "print(pdf_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3266944b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1905.05406_new.pdf', '1908.02426_new.pdf', '2103.07579_new.pdf', '2106.13112_new.pdf', '2109.05673_new.pdf']\n"
     ]
    }
   ],
   "source": [
    "# merge them into one pdf\n",
    "from PyPDF4 import PdfFileReader, PdfFileMerger, PdfFileWriter\n",
    "newfilelist = []\n",
    "for file in pdf_files:\n",
    "    with open(file, \"rb\") as pdf_file:\n",
    "        pdf = PdfFileReader(pdf_file)\n",
    "        page = pdf.getPage(0)\n",
    "    \n",
    "        writer = PdfFileWriter()\n",
    "        writer.addPage(page)\n",
    "\n",
    "        newfile = file.name[:-4]+'_new.pdf'\n",
    "        newfilelist.append(newfile)\n",
    "        with open(newfile, \"wb\") as output:\n",
    "             writer.write(output)\n",
    "print(newfilelist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5948300d",
   "metadata": {},
   "outputs": [],
   "source": [
    "merger = PdfFileMerger()\n",
    "\n",
    "for file_name in newfilelist:\n",
    "    merger.append(file_name)\n",
    "merger.write(\"new.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c81ad840",
   "metadata": {},
   "source": [
    "#### Take the abstracts of the first 20 papers in the given URL, translate them into Chinese, and write them into the file named 'my_abstract.txt’.  \n",
    "#### URL = https://openaccess.thecvf.com/ICCV2021?day=2021-10-12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "41af1bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import random\n",
    "from hashlib import md5\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "18f0581b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getUrl(url):\n",
    "    response = requests.get(url)\n",
    "    pdfUrl = []\n",
    "    count = 0\n",
    "    bs = BeautifulSoup(response.text,\"html.parser\")  # Parse the HTML content of the response using BeautifulSoup\n",
    "    pdfs = bs.find_all(href=re.compile('/content/.*html')) # Find all anchor tags with href attribute matching the specified pattern\n",
    "    names = []\n",
    "    for tag in pdfs:\n",
    "        if count < 20:  # Limit the count to 20\n",
    "            pdfUrl.append(\"https://openaccess.thecvf.com\"+tag.get(\"href\"))\n",
    "            count = count+1\n",
    "            # Extract the name from the href attribute and modify it before appending to the names list\n",
    "            names.append(re.split('[./]', tag.get(\"href\"))[-2][:-16].replace(\"_\",\" \")) \n",
    "        else: break\n",
    "    return names,pdfUrl\n",
    "\n",
    "\n",
    "def getAbstract(paperUrls):\n",
    "    abstract = []\n",
    "    for url in paperUrls:\n",
    "        response = requests.get(url)\n",
    "        bs = BeautifulSoup(response.text,\"html.parser\")\n",
    "        abstract.append(bs.find(id=\"abstract\").text.strip()) #get the content of id \"abstract\"\n",
    "    return abstract\n",
    "\n",
    "def translate_api(input_text):\n",
    "    # Set your own appid/appkey.\n",
    "    appid = '20231228001923565'\n",
    "    appkey = 'L0FUaa5T1WCTtXvCvW4R'\n",
    "    def make_md5(s, encoding='utf-8'):\n",
    "        return md5(s.encode(encoding)).hexdigest()\n",
    "    # For list of language codes, please refer to `https://api.fanyi.baidu.com/doc/21`\n",
    "    from_lang = 'en'\n",
    "    to_lang = 'zh'\n",
    "    endpoint = 'http://api.fanyi.baidu.com'\n",
    "    path = '/api/trans/vip/translate'\n",
    "    url = endpoint + path\n",
    "    \n",
    "    query = input_text\n",
    "    # Generate salt and sign\n",
    "    salt = random.randint(32768, 65536)\n",
    "    sign = make_md5(appid + query + str(salt) + appkey)\n",
    "    # Build request\n",
    "    headers = {'Content-Type': 'application/x-www-form-urlencoded'}\n",
    "    payload = {'appid': appid, 'q': query, 'from': from_lang, 'to': to_lang, 'salt': salt, 'sign': sign}\n",
    "    # Send request\n",
    "    r = requests.post(url, params=payload, headers=headers)\n",
    "    result = r.json()\n",
    "    print(result)\n",
    "\n",
    "    return result['trans_result'][0]['dst']\n",
    "\n",
    "def write2txt(path,title,contents):\n",
    "    with open(path,\"w\") as f:\n",
    "        for ti,con in zip(title,contents):\n",
    "            f.write(ti+\"\\n\")\n",
    "            f.write(con+\"\\n\")\n",
    "            f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "02c95dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "web = \"https://openaccess.thecvf.com/ICCV2021?day=2021-10-12\"\n",
    "name,pdfurls = getUrl(web)\n",
    "abstracts = getAbstract(pdfurls)\n",
    "ch_abstracts = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'from': 'en', 'to': 'zh', 'trans_result': [{'src': 'Learning-based image denoising methods have been bounded to situations where well-aligned noisy and clean images are given, or samples are synthesized from predetermined noise models, e.g., Gaussian. While recent generative noise modeling methods aim to simulate the unknown distribution of real-world noise, several limitations still exist. In a practical scenario, a noise generator should learn to simulate the general and complex noise distribution without using paired noisy and clean images. However, since existing methods are constructed on the unrealistic assumption of real-world noise, they tend to generate implausible patterns and cannot express complicated noise maps. Therefore, we introduce a Clean-to-Noisy image generation framework, namely C2N, to imitate complex real-world noise without using any paired examples. We construct the noise generator in C2N accordingly with each component of real-world noise characteristics to express a wide range of noise accurately. Combined wit', 'dst': '基于学习的图像去噪方法已经局限于这样的情况，即给出了对齐良好的有噪声和干净的图像，或者从预定的噪声模型（例如，高斯）合成样本。尽管最近的生成噪声建模方法旨在模拟真实世界噪声的未知分布，但仍存在一些局限性。在实际场景中，噪声生成器应该学会在不使用成对的噪声和干净图像的情况下模拟一般和复杂的噪声分布。然而，由于现有的方法是基于对真实世界噪声的不切实际的假设构建的，它们往往会生成令人难以置信的模式，并且无法表达复杂的噪声图。因此，我们引入了一种干净到噪声的图像生成框架，即C2N，以在不使用任何配对示例的情况下模拟复杂的真实世界噪声。我们在C2N中相应地构建了具有真实世界噪声特性的每个分量的噪声生成器，以准确地表达宽范围的噪声。综合智慧'}]}\n",
      "{'from': 'en', 'to': 'zh', 'trans_result': [{'src': 'Continually learning in the real world must overcome many challenges, among which noisy labels are a common and inevitable issue. In this work, we present a replay-based continual learning framework that simultaneously addresses both catastrophic forgetting and noisy labels for the first time. Our solution is based on two observations; (i) forgetting can be mitigated even with noisy labels via self-supervised learning, and (ii) the purity of the replay buffer is crucial. Building on this regard, we propose two key components of our method: (i) a self-supervised replay technique named Self-Replay, which can circumvent erroneous training signals arising from noisy labeled data, and (ii) the Self-Centered filter that maintains a purified replay buffer via centrality-based stochastic graph ensembles. The empirical results on MNIST, CIFAR-10, CIFAR-100, and WebVision with real-world noise demonstrate that our framework can maintain a highly pure replay buffer amidst noisy streamed data whil', 'dst': '在现实世界中不断学习必须克服许多挑战，其中嘈杂的标签是一个常见且不可避免的问题。在这项工作中，我们首次提出了一个基于重放的连续学习框架，该框架同时解决了灾难性遗忘和噪声标签。我们的解决方案基于两个观察结果；（i） 即使有噪声标签，也可以通过自监督学习来减轻遗忘，并且（ii）重放缓冲区的纯度至关重要。基于这一点，我们提出了我们方法的两个关键组成部分：（i）一种名为self replay的自监督重放技术，它可以绕过由噪声标记数据产生的错误训练信号，以及（ii）自中心滤波器，它通过基于中心性的随机图集合来维护纯化的重放缓冲区。MNIST、CIFAR-10、CIFAR-100和WebVision在真实世界噪声下的实验结果表明，我们的框架可以在噪声流数据中保持高度纯净的重放缓冲区'}]}\n",
      "{'from': 'en', 'to': 'zh', 'trans_result': [{'src': 'We introduce a method to render Neural Radiance Fields (NeRFs) in real time using PlenOctrees, an octree-based 3D representation which supports view-dependent effects. Our method can render 800x800 images at more than 150 FPS, which is over 3000 times faster than conventional NeRFs. We do so without sacrificing quality while preserving the ability of NeRFs to perform free-viewpoint rendering of scenes with arbitrary geometry and view-dependent effects. Real-time performance is achieved by pre-tabulating the NeRF into a PlenOctree. In order to preserve view-dependent effects such as specularities, we factorize the appearance via closed-form spherical basis functions. Specifically, we show that it is possible to train NeRFs to predict a spherical harmonic representation of radiance, removing the viewing direction as an input to the neural network. Furthermore, we show that PlenOctrees can be directly optimized to further minimize the reconstruction loss, which leads to equal or better qu', 'dst': '我们介绍了一种使用PlenOctrees实时渲染神经辐射场（NeRF）的方法，PlenOctres是一种基于八叉树的3D表示，支持视图相关效果。我们的方法可以以超过150 FPS的速度渲染800x800张图像，这比传统的NeRF快3000多倍。我们在不牺牲质量的情况下这样做，同时保留了NeRF对具有任意几何体和视图相关效果的场景执行自由视点渲染的能力。实时性能是通过将NeRF预先制表为PlenOctree来实现的。为了保持与视图相关的效果，如镜面反射，我们通过闭合形式的球面基函数来分解外观。具体来说，我们证明了训练NeRF来预测辐射的球面谐波表示是可能的，去除了作为神经网络输入的观看方向。此外，我们还证明了PlenOctrees可以直接优化，以进一步最小化重建损失，从而获得相等或更好的qu'}]}\n",
      "{'from': 'en', 'to': 'zh', 'trans_result': [{'src': 'Deep neural networks (DNNs) for the semantic segmentation of images are usually trained to operate on a predefined closed set of object classes. This is in contrast to the \"\"open world\"\" setting where DNNs are envisioned to be deployed to. From a functional safety point of view, the ability to detect so-called \"\"out-of-distribution\"\" (OoD) samples, i.e., objects outside of a DNN\\'s semantic space, is crucial for many applications such as automated driving. A natural baseline approach to OoD detection is to threshold on the pixel-wise softmax entropy. We present a two-step procedure that significantly improves that approach. Firstly, we utilize samples from the COCO dataset as OoD proxy and introduce a second training objective to maximize the softmax entropy on these samples. Starting from pretrained semantic segmentation networks we re-train a number of DNNs on different in-distribution datasets and consistently observe improved OoD detection performance when evaluating on completely d', 'dst': '用于图像语义分割的深度神经网络（DNN）通常被训练为对预定义的对象类的闭合集进行操作。这与DNN被设想部署到的“开放世界”设置形成了鲜明对比。从功能安全的角度来看，检测所谓的“分布外”（OoD）样本（即DNN语义空间之外的对象）的能力对自动驾驶等许多应用至关重要。OoD检测的一种自然基线方法是对像素方向的softmax熵进行阈值设置。我们提出了一个两步程序，大大改进了这种方法。首先，我们利用来自COCO数据集的样本作为OoD代理，并引入第二个训练目标来最大化这些样本的softmax熵。从预先训练的语义分割网络开始，我们在不同的分布数据集上重新训练了许多DNN，并在完全d上评估时一致地观察到改进的OoD检测性能'}]}\n",
      "{'from': 'en', 'to': 'zh', 'trans_result': [{'src': 'RGB-D saliency detection has attracted increasing attention, due to its effectiveness and the fact that depth cues can now be conveniently captured. Existing works often focus on learning a shared representation through various fusion strategies, with few methods explicitly considering how to preserve modality-specific characteristics. In this paper, taking a new perspective, we propose a specificity-preserving network for RGB-D saliency detection, which benefits saliency detection performance by exploring both the shared information and modality-specific properties (e.g., specificity). Specifically, two modality-specific networks and a shared learning network are adopted to generate individual and shared saliency maps. A cross-enhanced integration module (CIM) is proposed to fuse cross-modal features in the shared learning network, which are then propagated to the next layer for integrating cross-level information. Besides, we propose a multi-modal feature aggregation (MFA) module to', 'dst': 'RGB-D显著性检测由于其有效性和现在可以方便地捕捉深度线索的事实而引起了越来越多的关注。现有的工作通常侧重于通过各种融合策略学习共享表示，很少有方法明确考虑如何保持模态特定的特征。在本文中，从一个新的角度，我们提出了一种用于RGB-D显著性检测的特异性保留网络，该网络通过探索共享信息和模态特定属性（例如，特异性）来提高显著性检测性能。具体而言，采用两个模态特定网络和一个共享学习网络来生成个体显著性图和共享显著性图。提出了一种交叉增强集成模块（CIM）来融合共享学习网络中的跨模态特征，然后将其传播到下一层以集成跨级别信息。此外，我们提出了一个多模态特征聚合（MFA）模块来实现'}]}\n",
      "{'from': 'en', 'to': 'zh', 'trans_result': [{'src': 'Visual grounding on 3D point clouds is an emerging vision and language task that benefits various applications in understanding the 3D visual world. By formulating this task as a grounding-by-detection problem, lots of recent works focus on how to exploit more powerful detectors and comprehensive language features, but (1) how to model complex relations for generating context-aware object proposals and (2) how to leverage proposal relations to distinguish the true target object from similar proposals are not fully studied yet. Inspired by the well-known transformer architecture, we propose a relation-aware visual grounding method on 3D point clouds, named as 3DVG-Transformer, to fully utilize the contextual clues for relationenhanced proposal generation and cross-modal proposal disambiguation, which are enabled by a newly designed coordinate-guided contextual aggregation (CCA) module in the object proposal generation stage, and a multiplex attention (MA) module in the cross-modal featu', 'dst': '三维点云视觉基础是一项新兴的视觉和语言任务，有利于理解三维视觉世界的各种应用。通过将这项任务定义为基于检测的问题，最近的许多工作都集中在如何利用更强大的检测器和全面的语言特征上，但（1）如何对复杂关系进行建模以生成上下文感知的对象建议，以及（2）如何利用建议关系将真实的目标对象与类似的建议区分开来，还没有得到充分的研究。受众所周知的transformer架构的启发，我们提出了一种基于3D点云的关系感知视觉基础方法，称为3DVG transformer，以充分利用上下文线索进行关系增强建议生成和跨模态建议消歧，这是通过在对象建议生成阶段新设计的坐标引导上下文聚合（CCA）模块和在跨模态建议生成阶段的多重注意力（MA）模块实现的'}]}\n",
      "{'from': 'en', 'to': 'zh', 'trans_result': [{'src': 'The non-local self-similarity property of natural images has been exploited extensively for solving various image processing problems. When it comes to video sequences, harnessing this force is even more beneficial due to the temporal redundancy. In the context of image and video denoising, many classically-oriented algorithms employ self-similarity, splitting the data into overlapping patches, gathering groups of similar ones and processing these together somehow. With the emergence of convolutional neural networks (CNN), the patch-based framework has been abandoned. Most CNN denoisers operate on the whole image, leveraging non-local relations only implicitly by using a large receptive field. This work proposes a novel approach for leveraging self-similarity in the context of video denoising, while still relying on a regular convolutional architecture. We introduce a concept of patch-craft frames - artificial frames that are similar to the real ones, built by tiling matched patches. O', 'dst': '自然图像的非局部自相似性已被广泛用于解决各种图像处理问题。当涉及到视频序列时，由于时间冗余，利用这种力量甚至更有益。在图像和视频去噪的背景下，许多面向经典的算法采用自相似性，将数据分割成重叠的补丁，收集相似的补丁组，并以某种方式将其处理在一起。随着卷积神经网络（CNN）的出现，基于补丁的框架已经被抛弃。大多数CNN去噪器对整个图像进行操作，仅通过使用大的接受场来隐含地利用非局部关系。这项工作提出了一种在视频去噪的背景下利用自相似性的新方法，同时仍然依赖于规则的卷积架构。我们引入了贴片工艺框架的概念——通过平铺匹配的贴片构建的与真实框架相似的人造框架。O'}]}\n",
      "{'from': 'en', 'to': 'zh', 'trans_result': [{'src': \"Text-based image retrieval has seen considerable progress in recent years. However, the performance of existing methods suffers in real life since the user is likely to provide an incomplete description of an image, which often leads to results filled with false positives that fit the incomplete description. In this work, we introduce the partial-query problem and extensively analyze its influence on text-based image retrieval. Previous interactive methods tackle the problem by passively receiving users' feedback to supplement the incomplete query iteratively, which is time-consuming and requires heavy user effort. Instead, we propose a novel retrieval framework that conducts the interactive process in an Ask-and-Confirm fashion, where AI actively searches for discriminative details missing in the current query, and users only need to confirm AI's proposal. Specifically, we propose an object-based interaction to make the interactive retrieval more user-friendly and present a reinforcem\", 'dst': '近年来，基于文本的图像检索取得了长足的进步。然而，现有方法的性能在现实生活中受到影响，因为用户可能提供图像的不完整描述，这通常导致结果充满了符合不完整描述的误报。在这项工作中，我们介绍了部分查询问题，并广泛分析了它对基于文本的图像检索的影响。以前的交互式方法通过被动地接收用户的反馈来迭代地补充不完整的查询来解决这个问题，这是耗时的，并且需要大量的用户努力。相反，我们提出了一种新颖的检索框架，该框架以询问和确认的方式进行交互过程，人工智能主动搜索当前查询中缺失的歧视性细节，用户只需要确认人工智能的提议。具体来说，我们提出了一种基于对象的交互，使交互式检索更用户友好，并呈现出一种增强'}]}\n",
      "{'from': 'en', 'to': 'zh', 'trans_result': [{'src': 'The successful deployment of artificial intelligence (AI) in many domains from healthcare to hiring requires their responsible use, particularly in model explanations and privacy. Explainable artificial intelligence (XAI) provides more information to help users to understand model decisions, yet this additional knowledge exposes additional risks for privacy attacks. Hence, providing explanation harms privacy. We study this risk for image-based model inversion attacks and identified several attack architectures with increasing performance to reconstruct private image data from model explanations. We have developed several multi-modal transposed CNN architectures that achieve significantly higher inversion performance than using the target model prediction only. These XAI-aware inversion models were designed to exploit the spatial knowledge in image explanations. To understand which explanations have higher privacy risk, we analyzed how various explanation types and factors influence inv', 'dst': '人工智能（AI）在从医疗保健到招聘的许多领域的成功部署需要负责任地使用，特别是在模型解释和隐私方面。可解释人工智能（XAI）提供了更多信息来帮助用户理解模型决策，但这些额外的知识暴露了隐私攻击的额外风险。因此，提供解释会损害隐私。我们研究了基于图像的模型反转攻击的这种风险，并确定了几种性能不断提高的攻击架构，以从模型解释中重建私人图像数据。我们已经开发了几种多模态转置CNN架构，它们实现了比仅使用目标模型预测高得多的反演性能。这些XAI感知反演模型旨在利用图像解释中的空间知识。为了了解哪些解释具有更高的隐私风险，我们分析了各种解释类型和因素如何影响inv'}]}\n",
      "{'from': 'en', 'to': 'zh', 'trans_result': [{'src': 'Training a neural network model for recognizing multiple labels associated with an image, including identifying unseen labels, is challenging, especially for images that portray numerous semantically diverse labels. As challenging as this task is, it is an essential task to tackle since it represents many real-world cases, such as image retrieval of natural images. We argue that using a single embedding vector to represent an image, as commonly practiced, is not sufficient to rank both relevant seen and unseen labels accurately. This study introduces an end-to-end model training for multi-label zero-shot learning that supports the semantic diversity of the images and labels. We propose to use an embedding matrix having principal embedding vectors trained using a tailored loss function. In addition, during training, we suggest up-weighting in the loss function image samples presenting higher semantic diversity to encourage the diversity of the embedding matrix. Extensive experiments sho', 'dst': '训练用于识别与图像相关联的多个标签（包括识别看不见的标签）的神经网络模型是具有挑战性的，尤其是对于描绘许多语义不同标签的图像而言。尽管这项任务具有挑战性，但它是一项需要解决的重要任务，因为它代表了许多现实世界的情况，例如自然图像的图像检索。我们认为，像通常实践的那样，使用单个嵌入向量来表示图像不足以准确地对相关的可见和不可见标签进行排序。本研究介绍了一种用于多标签零样本学习的端到端模型训练，该训练支持图像和标签的语义多样性。我们建议使用具有使用定制损失函数训练的主嵌入向量的嵌入矩阵。此外，在训练过程中，我们建议在表现出更高语义多样性的损失函数图像样本中增加权重，以鼓励嵌入矩阵的多样性。广泛的实验sho'}]}\n",
      "{'from': 'en', 'to': 'zh', 'trans_result': [{'src': 'Existing change captioning studies have mainly focused on a single change. However, detecting and describing multiple changed parts in image pairs is essential for enhancing adaptability to complex scenarios. We solve the above issues from three aspects: (i) We propose a simulation-based multi-change captioning dataset; (ii) We benchmark existing state-of-the-art methods of single change captioning on multi-change captioning; (iii) We further propose Multi-Change Captioning transformers (MCCFormers) that identify change regions by densely correlating different regions in image pairs and dynamically determines the related change regions with words in sentences. The proposed method obtained the highest scores on four conventional change captioning evaluation metrics for multi-change captioning. Additionally, our proposed method can separate attention maps for each change and performs well with respect to change localization. Moreover, the proposed framework outperformed the previous stat', 'dst': '现有的更改字幕研究主要集中在单个更改上。然而，检测和描述图像对中的多个变化部分对于增强对复杂场景的适应性至关重要。我们从三个方面解决了上述问题：（i）提出了一个基于模拟的多变化字幕数据集；（ii）我们将现有最先进的单次更改字幕方法与多次更改字幕进行比较；（iii）我们进一步提出了多变化字幕转换器（MCCFormer），其通过密集地关联图像对中的不同区域来识别变化区域，并动态地确定与句子中的单词相关的变化区域。所提出的方法在多变化字幕的四个传统变化字幕评估指标上获得了最高分数。此外，我们提出的方法可以为每个变化分离注意力图，并且在变化定位方面表现良好。此外，所提出的框架优于之前的统计数据'}]}\n",
      "{'from': 'en', 'to': 'zh', 'trans_result': [{'src': \"Point clouds acquired from scanning devices are often perturbed by noise, which affects downstream tasks such as surface reconstruction and analysis. The distribution of a noisy point cloud can be viewed as the distribution of a set of noise-free samples p(x) convolved with some noise model n, leading to (p * n)(x) whose mode is the underlying clean surface. To denoise a noisy point cloud, we propose to increase the log-likelihood of each point from p * n via gradient ascent---iteratively updating each point's position. Since p * n is unknown at test-time, and we only need the score (i.e., the gradient of the log-probability function) to perform gradient ascent, we propose a neural network architecture to estimate the score of p * n given only noisy point clouds as input. We derive objective functions for training the network and develop a denoising algorithm leveraging on the estimated scores. Experiments demonstrate that the proposed model outperforms state-of-the-art methods under a\", 'dst': '从扫描设备获取的点云经常受到噪声的干扰，这会影响表面重建和分析等下游任务。噪声点云的分布可以看作是一组无噪声样本p（x）与某个噪声模型n卷积后的分布，从而得到（p*n）（x），其模式是下面的清洁表面。为了对噪声点云进行去噪，我们建议通过梯度上升来增加每个点的对数似然性，即迭代更新每个点的位置。由于p*n在测试时是未知的，并且我们只需要分数（即对数概率函数的梯度）来执行梯度上升，因此我们提出了一种神经网络架构来估计p*n的分数，仅给定噪声点云作为输入。我们推导出用于训练网络的目标函数，并利用估计的分数开发去噪算法。实验表明，在'}]}\n",
      "{'from': 'en', 'to': 'zh', 'trans_result': [{'src': 'Unprecedented access to multi-temporal satellite imagery has opened new perspectives for a variety of Earth observation tasks. Among them, pixel-precise panoptic segmentation of agricultural parcels has major economic and environmental implications. While researchers have explored this problem for single images, we argue that the complex temporal patterns of crop phenology are better addressed with temporal sequences of images. In this paper, we present the first end-to-end, single-stage method for panoptic segmentation of Satellite Image Time Series (SITS). This module can be combined with our novel image sequence encoding network which relies on temporal self-attention to extract rich and adaptive multi-scale spatio-temporal features. We also introduce PASTIS, the first open-access SITS dataset with panoptic annotations. We demonstrate the superiority of our encoder for semantic segmentation against multiple competing network architectures, and set up the first state-of-the-art of pa', 'dst': '前所未有地获取多时相卫星图像为各种地球观测任务开辟了新的视角。其中，农业地块的像素精确全景分割具有重要的经济和环境影响。虽然研究人员已经在单个图像中探索了这个问题，但我们认为，通过图像的时间序列可以更好地解决作物表型的复杂时间模式。在本文中，我们提出了第一种端到端的单阶段卫星图像时间序列全景分割方法。该模块可以与我们新颖的图像序列编码网络相结合，该网络依赖于时间自注意来提取丰富且自适应的多尺度时空特征。我们还介绍了PASTIS，这是第一个具有全景注释的开放访问SITS数据集。我们展示了我们的编码器在语义分割方面相对于多种竞争网络架构的优势，并建立了第一个最先进的'}]}\n",
      "{'from': 'en', 'to': 'zh', 'trans_result': [{'src': 'Although convolutional neural networks (CNNs) have achieved great success in computer vision, this work investigates a simpler, convolution-free backbone network useful for many dense prediction tasks. Unlike the recently-proposed Vision Transformer (ViT) that was designed for image classification specifically, we introduce the Pyramid Vision Transformer (PVT), which overcomes the difficulties of porting Transformer to various dense prediction tasks. PVT has several merits compared to current state of the arts. (1) Different from ViT that typically yields low-resolution outputs and incurs high computational and memory costs, PVT not only can be trained on dense partitions of an image to achieve high output resolution, which is important for dense prediction, but also uses a progressive shrinking pyramid to reduce the computations of large feature maps. (2) PVT inherits the advantages of both CNN and Transformer, making it a unified backbone for various vision tasks without convolutions', 'dst': '尽管卷积神经网络（CNNs）在计算机视觉领域取得了巨大成功，但这项工作研究了一种更简单、无卷积的主干网络，可用于许多密集的预测任务。与最近提出的专门为图像分类设计的视觉转换器（ViT）不同，我们引入了金字塔视觉转换器（PVT），它克服了将转换器移植到各种密集预测任务的困难。与现有技术相比，PVT有几个优点。（1） 与通常产生低分辨率输出并导致高计算和内存成本的ViT不同，PVT不仅可以在图像的密集分区上进行训练以实现高输出分辨率，这对密集预测很重要，而且还使用渐进收缩金字塔来减少大特征图的计算。（2） PVT继承了CNN和Transformer的优势，使其成为无卷积的各种视觉任务的统一骨干'}]}\n",
      "{'from': 'en', 'to': 'zh', 'trans_result': [{'src': 'Causally-taken images often suffer from flare artifacts, due to the unintended reflections and scattering of light inside the camera. However, as flares may appear in a variety of shapes, positions, and colors, detecting and removing them entirely from an image is very challenging. Existing methods rely on predefined intensity and geometry priors of flares, and may fail to distinguish the difference between light sources and flare artifacts. We observe that the conditions of the light source in the image play an important role in the resulting flares. In this paper, we present a deep framework with light source aware guidance for single-image flare removal (SIFR). In particular, we first detect the light source regions and the flare regions separately, and then remove the flare artifacts based on the light source aware guidance. By learning the underlying relationships between the two types of regions, our approach can remove different kinds of flares from the image. In addition, inste', 'dst': '由于相机内部光线的意外反射和散射，因果拍摄的图像通常会出现光斑伪影。然而，由于光斑可能以各种形状、位置和颜色出现，因此从图像中完全检测和去除光斑是非常具有挑战性的。现有的方法依赖于光斑的预定义强度和几何先验，并且可能无法区分光源和光斑伪影之间的差异。我们观察到，图像中光源的条件在产生耀斑中起着重要作用。在本文中，我们提出了一个具有光源感知制导的深度框架，用于单图像光斑去除（SIFR）。特别地，我们首先分别检测光源区域和光斑区域，然后基于光源感知引导来去除光斑伪影。通过学习这两种区域之间的基本关系，我们的方法可以从图像中去除不同类型的光斑。此外，inste'}]}\n",
      "{'from': 'en', 'to': 'zh', 'trans_result': [{'src': 'Dynamic inference networks, aimed at promoting computational efficiency, go along an adaptive executing path for a given sample. Prevalent methods typically assign a router for each convolutional block and sequentially make block-by-block executing decisions, without considering the relations during the dynamic inference. In this paper, we model the relations for dynamic inference from two aspects: the routers and the samples. We design a novel type of router called the relational router to model the relations among routers for a given sample. In principle, the current relational router aggregates the contextual features of preceding routers by graph convolution and propagates its router features to subsequent ones, making the executing decision for the current block in a long-range manner. Furthermore, we model the relation between samples by introducing a Sample Relation Module (SRM), encouraging correlated samples to go along correlated executing paths. As a whole, we call our metho', 'dst': '动态推理网络旨在提高计算效率，对给定的样本采用自适应执行路径。主流方法通常为每个卷积块分配一个路由器，并顺序地逐块执行决策，而不考虑动态推理过程中的关系。本文从路由器和样本两个方面对动态推理的关系进行了建模。我们设计了一种称为关系路由器的新型路由器，以对给定样本的路由器之间的关系进行建模。原则上，当前关系路由器通过图卷积聚合先前路由器的上下文特征，并将其路由器特征传播到后续路由器，从而以长距离的方式为当前块做出执行决策。此外，我们通过引入样本关系模块（SRM）来对样本之间的关系进行建模，鼓励相关样本沿着相关的执行路径前进。作为一个整体，我们称我们的方法'}]}\n",
      "{'from': 'en', 'to': 'zh', 'trans_result': [{'src': 'This paper introduces an unsupervised loss for training parametric deformation shape generators. The key idea is to enforce the preservation of local rigidity among the generated shapes. Our approach builds on a local approximation of the as-rigid-as possible (or ARAP) deformation energy. We show how to develop the unsupervised loss via a spectral decomposition of the Hessian of the ARAP loss. Our loss nicely decouples pose and shape variations through a robust norm. The loss admits simple closed-form expressions. It is easy to train and can be plugged into any standard generation models, e.g., VAE and GAN. Experimental results show that our approach outperforms existing shape generation approaches considerably across various datasets such as DFAUST, Animal, and Bone.', 'dst': '本文介绍了一种用于训练参数变形形状生成器的无监督损失。关键思想是在生成的形状中强制保持局部刚性。我们的方法建立在尽可能刚性（或ARAP）变形能量的局部近似上。我们展示了如何通过ARAP损失的Hessian谱分解来发展无监督损失。我们的损失通过一个稳健的范数很好地解耦了姿势和形状的变化。损失承认了简单的封闭形式表达。它易于训练，并且可以插入任何标准的一代模型，例如VAE和GAN。实验结果表明，在DFAUST、Animal和Bone等各种数据集中，我们的方法显著优于现有的形状生成方法。'}]}\n",
      "{'from': 'en', 'to': 'zh', 'trans_result': [{'src': 'We present a two-stage learning framework for weakly supervised object localization (WSOL). While most previous efforts rely on high-level feature based CAMs (Class Activation Maps), this paper proposes to localize objects using the low-level feature based activation maps. In the first stage, an activation map generator produces activation maps based on the low-level feature maps in the classifier, such that rich contextual object information is included in an online manner. In the second stage, we employ an evaluator to evaluate the activation maps predicted by the activation map generator. Based on this, we further propose a weighted entropy loss, an attentive erasing, and an area loss to drive the activation map generator to substantially reduce the uncertainty of activations between object and background, and explore less discriminative regions. Based on the low-level object information preserved in the first stage, the second stage model gradually generates a well-separated, compl', 'dst': '我们提出了一个用于弱监督对象定位（WSOL）的两阶段学习框架。虽然以前的大多数工作都依赖于基于高级特征的CAM（类激活映射），但本文提出使用基于低级特征的激活映射来定位对象。在第一阶段中，激活图生成器基于分类器中的低级特征图来生成激活图，使得以在线方式包括丰富的上下文对象信息。在第二阶段，我们使用评估器来评估由激活图生成器预测的激活图。在此基础上，我们进一步提出了加权熵损失、注意擦除和面积损失，以驱动激活图生成器大幅降低对象和背景之间激活的不确定性，并探索歧视性较小的区域。第二阶段模型在第一阶段保存的低层次对象信息的基础上，逐渐生成一个分离良好的、复杂的、可扩展的对象'}]}\n",
      "{'from': 'en', 'to': 'zh', 'trans_result': [{'src': 'Research in unpaired video translation has mainly focused on short-term temporal consistency by conditioning on neighboring frames. However for transfer from simulated to photorealistic sequences, available information on the underlying geometry offers potential for achieving global consistency across views. We propose a novel approach which combines unpaired image translation with neural rendering to transfer simulated to photorealistic surgical abdominal scenes. By introducing global learnable textures and a lighting-invariant view-consistency loss, our method produces consistent translations of arbitrary views and thus enables long-term consistent video synthesis. We design and test our model to generate video sequences from minimally-invasive surgical abdominal scenes. Because labeled data is often limited in this domain, photorealistic data where ground truth information from the simulated domain is preserved is especially relevant. By extending existing image-based methods to vie', 'dst': '非配对视频翻译的研究主要集中在通过对相邻帧进行条件处理来实现短期时间一致性。然而，对于从模拟序列到照片真实感序列的转换，有关基础几何图形的可用信息为实现视图之间的全局一致性提供了潜力。我们提出了一种新的方法，该方法将不成对的图像转换与神经渲染相结合，将模拟的外科腹部场景转换为真实感外科腹部场景。通过引入全局可学习纹理和照明不变的视图一致性损失，我们的方法可以产生任意视图的一致翻译，从而实现长期一致的视频合成。我们设计并测试了我们的模型，以从微创手术腹部场景中生成视频序列。由于标记数据在该领域中通常是有限的，因此保存了来自模拟领域的地面实况信息的真实照片数据尤其相关。通过将现有的基于图像的方法扩展到vie'}]}\n",
      "{'from': 'en', 'to': 'zh', 'trans_result': [{'src': 'Online semantic segmentation on a time series of point cloud frames is an essential task in autonomous driving. Existing models focus on single-frame segmentation, which cannot achieve satisfactory segmentation accuracy and offer unstably flicker among frames. In this paper, we propose a light-weight semantic segmentation framework for large-scale point cloud series, called TempNet, which can improve both the accuracy and the stability of existing semantic segmentation models by combining a novel frame aggregation scheme. To be computational cost efficient, feature extraction and aggregation are only conducted on a small portion of key frames via a temporal feature aggregation (TFA) network using an attentional pooling mechanism, and such enhanced features are propagated to the intermediate non-key frames. To avoid information loss from non-key frames, a partial feature update (PFU) network is designed to partially update the propagated features with the local features extracted on a n', 'dst': '点云帧时间序列的在线语义分割是自动驾驶中的一项重要任务。现有的模型侧重于单帧分割，不能达到令人满意的分割精度，并且在帧之间提供不稳定的闪烁。在本文中，我们提出了一种用于大规模点云序列的轻量级语义分割框架，称为TempNet，通过结合一种新的帧聚合方案，可以提高现有语义分割模型的准确性和稳定性。为了计算成本高效，通过使用注意力池机制的时间特征聚合（TFA）网络，仅对关键帧的一小部分进行特征提取和聚合，并且这种增强的特征被传播到中间非关键帧。为了避免非关键帧的信息丢失，设计了一个部分特征更新（PFU）网络，用在n帧上提取的局部特征对传播的特征进行部分更新'}]}\n"
     ]
    }
   ],
   "source": [
    "for ab in abstracts:\n",
    "    temp = translate_api(ab)\n",
    "    ch_abstracts.append(temp)\n",
    "    time.sleep(1)\n",
    "write2txt(\"./my_abstract.txt\",name,ch_abstracts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
